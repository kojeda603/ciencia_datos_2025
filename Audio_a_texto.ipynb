{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP0u67tEJE/wUkf22l/yfuM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kojeda603/ciencia_datos_2025/blob/main/Audio_a_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar dependencias\n",
        "!pip install openai-whisper yt-dlp\n",
        "!sudo apt update && sudo apt install -y ffmpeg\n",
        "\n",
        "import whisper\n",
        "import os\n",
        "\n",
        "# 1. Descargar el video de YouTube (solo audio en mp3 para hacerlo más liviano)\n",
        "url = \"https://www.youtube.com/watch?v=zuZ2zaotrJs\"\n",
        "!yt-dlp -x --audio-format mp3 -o \"video.mp3\" $url\n",
        "\n",
        "# 2. Cargar el modelo Whisper\n",
        "model = whisper.load_model(\"small\")  # puedes usar \"medium\" para más precisión\n",
        "\n",
        "# 3. Transcribir el audio (en inglés → texto en inglés)\n",
        "result = model.transcribe(\"video.mp3\", language=\"en\")\n",
        "\n",
        "# 4. Mostrar transcripción\n",
        "print(result[\"text\"])\n",
        "\n",
        "# 5. Guardar transcripción en un archivo\n",
        "with open(\"transcription_en.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result[\"text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUXEHPTJDNOp",
        "outputId": "4957a284-a2ca-494e-d4ed-764450d90655"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.12/dist-packages (20250625)\n",
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.12/dist-packages (2025.8.27)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 36 not upgraded.\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=zuZ2zaotrJs\n",
            "[youtube] zuZ2zaotrJs: Downloading webpage\n",
            "[youtube] zuZ2zaotrJs: Downloading tv simply player API JSON\n",
            "[youtube] zuZ2zaotrJs: Downloading tv client config\n",
            "[youtube] zuZ2zaotrJs: Downloading tv player API JSON\n",
            "[info] zuZ2zaotrJs: Downloading 1 format(s): 251\n",
            "[download] video.mp3 has already been downloaded\n",
            "[ExtractAudio] Not converting audio video.mp3; file is already in target format mp3\n",
            " Okay, hi everyone. It's really nice to be here. I want to thank everyone for setting this up, for organizing this, for giving me this honorary degree. It's actually extremely meaningful to receive this honorary degree. Almost to this day, 20 years ago, I received my bachelor's degree from the University of Toronto in this exact hall. And actually at this point now I have, this would be my fourth degree from the University of Toronto. And I had a really extremely wonderful time here. I spent a total of 10 years, I did my undergrad degree and I learned a lot, and I'm also a graduate student here. And it was really wonderful. I was able to go deep in my, whatever I was interested in, and to really become a researcher. It was really wonderful to study with Jeff Hinton actually. The fact that Jeff Hinton was in this university was one of my life's great strokes of luck. And I feel a lot of gratitude to the university. I feel like I couldn't have asked for a better way to become educated, mature, become a scientist. And also the University of Toronto, when I was a student here, we were doing the best AI research out of anywhere. It was the most revolutionary ideas, the most exciting work. And I feel very lucky that I was able to contribute to it already in grad school as a student. That was a long time ago. And the way I understand it, in a convocation speech, one is supposed to provide sagacious advice, and I'll do a little bit of that, but only a bit, because this one will, this speech will be a little bit different. You know, the, I will offer one bit of a useful state of mind, which if one adopts it, makes everything much easier, which is to accept reality as it is and to try not to regret the past and try to improve the situation. And the reason I say it is because it's so hard to adopt it. It's so easy to think, oh, like some bad past decision or bad stroke of luck, something happened, something's unfair, and you can just spend, it's so easy to spend so much time thinking like this. While it's just so much better and more productive to say, okay, things are the way they are, what's the next best step? And I find that whenever I do this myself, everything works out so much better, but it's hard, it's hard, it's a constant struggle with one's emotion. And that's why I mention it to you, perhaps some of you will adopt it yourself. This is a reminder to adopt this mindset as best as one can, and also a reminder for myself, constant struggle. But this aside, the reason it's not going to be the most conventional convocation speech is because there is something a little different going on. Right now, you all leave, we all leave in the most unusual time ever. And this is something that people might say often, but I think it's actually true this time. And the reason it's true this time is because of AI, right, obviously. I mean, from what I hear, the AI of today has already changed what it means to be a student by a pretty considerable degree. That's what I sense, and I think it's true. But of course, the impact of AI goes beyond that. What happens to the kind of work we do? Well, it's starting to change a little bit in some unknown and unpredictable ways. And some work may feel it sooner, some work might feel it later. With today's AI, you can go on Twitter and you can look at what AI can do and what people say. And you might feel a little bit of that. You wonder, hey, which skills are useful, which ones will be less useful? So you've got these questions going on. And so you can say that the current level of challenge is how will it affect work and our careers? But the thing, the real challenge with AI is that it's really unprecedented and really extreme. And it's going to be very different in the future compared to the way it is today. Like, you know, we've all seen AI. We've all spoken to a computer and a computer has spoken back to us, which is a new thing. This would not do this in the past, but now they do. So you speak to a computer and it understands you and it speaks back to you. And it also does it in voice and it writes some code. It's pretty crazy. But there are so many things it cannot do as well and it's so deficient. So you can say it still needs to catch up on a lot of things. But it's evocative. It's good enough that you can ask yourself, you could imagine, okay, fine. In some number of years, some people say it's in three, some people say it's in five, 10 numbers have been thrown around. It's a bit hard to predict the future. But slowly, but surely, or maybe not so slowly AI will keep getting better and the day will come when I will do all of our, all the things that we can do. Not just some of them, but all of them, anything which I can learn, anything which any, any one of you can learn the AI could do as well. How do we know this, by the way? How can I be so sure? How can I be so sure of that? The reason is that all of us have a brain and the brain is a biological computer. That's why. We have a brain, the brain is a biological computer. So why can't a digital computer, a digital brain do the same things? This is the one sentence summary for why AI will be able to do all those things because we have a brain and the brain is a biological computer. And so you can start asking yourselves, what's going to happen? What's going to happen when computers can do all of our jobs, right? Those are really big questions. Those are dramatic questions. And right now, like you start thinking about it a little bit, you go, gosh, that's really intense. But it's actually only part of the intensity because what's going to happen? What, what will be the collective V want to use these AIs for? Do more work, grow the economy, do R and D, do AI research. So then the rate of progress will become really extremely fast for some time at least. These are such extreme things. These are such unimaginable things. So right now I'm trying to pull you into that a little bit into this headspace of this really extreme and radical future that AI creates. But it's also very difficult to imagine. It's very, very difficult to imagine. It's very difficult to internalize and to really believe on an emotional level. Even I struggle with it. And yet the logic seems to dictate that this very likely should happen. So what does one do in such a world? You know, there is a quote which is like this, which goes like this. It says, you may not take interest in politics, but politics will take interest in you. So the same applies to AI many times over. And in particular, I think that by simply using AI and looking at what the best AI of today can do, you get an intuition. You get an intuition. And as AI continues to improve in one year, in two years, in three years, the intuition will become stronger. And a lot of the things that you're talking about now, they will become much more real. They'll become less imaginary. And then of the day, no amount of essays and explanations can compete with what we see with our own senses, with our own two eyes. And especially with AI, the very smart, super intelligent AI in the future, there will be very profound issues about making sure that they say what they say and not pretend to be something else. And I'm really condensing a lot into a small amount of information here, time here. But overall, by simply looking at what AI can do, not ignoring it, when the time comes that will generate the energy that's required to overcome the huge challenge that AI will pose. And the challenge that AI poses in some sense is the greatest challenge of humanity ever. And overcoming it, you'll also bring the greatest reward. And in some sense, whether you like it or not, your life is going to be affected by AI to a great extent. And so looking at it, paying attention, and then generating the energy to solve the problems that will come up, that's going to be the main thing. And I'll stop here. Thank you so much. Thank you.\n"
          ]
        }
      ]
    }
  ]
}